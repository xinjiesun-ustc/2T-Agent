import openai
from openai import OpenAI
from config import OPENAI_API_KEY, OPENAI_API_BASE,SIM_PARAMS

# class LLMClient:
#     """
#     LLM æ¥å£è°ƒç”¨å°è£…æ¨¡å—
#     ç”¨äºç»Ÿä¸€å¤„ç† GPT / DeepSeek ç­‰æ¨¡å‹çš„è°ƒç”¨
#     """
#     def __init__(self):
#         openai.api_key = OPENAI_API_KEY
#         self.client = OpenAI(api_key=OPENAI_API_KEY,base_url=OPENAI_API_BASE)
#
#         mtype = SIM_PARAMS['gpt_type']
#         if mtype == 0:
#             self.model = 'gpt-4o-mini-2024-07-18'
#             # self.model = 'gpt-3.5-turbo-1106'
#         elif mtype == 1:
#             self.model = 'gpt-4-1106-preview'
#         # elif mtype == 2:
#         #     self.model = 'deepseek-chat'
#         else:
#             raise ValueError(f"Unsupported gpt_type: {mtype}")
#
#     def call(self, messages):
#         print(messages)
#         resp = self.client.chat.completions.create(
#             model=self.model,
#             messages=messages,
#             temperature=0,
#             timeout=120,
#             max_tokens=2048
#         )
#         return resp.choices[0].message.content

import time
import openai
from openai import OpenAI
from openai import RateLimitError, APIError, Timeout, APIConnectionError

class LLMClient:
    """
    LLM æ¥å£è°ƒç”¨å°è£…æ¨¡å—
    ç”¨äºç»Ÿä¸€å¤„ç† GPT / DeepSeek ç­‰æ¨¡å‹çš„è°ƒç”¨
    """
    def __init__(self):
        openai.api_key = OPENAI_API_KEY
        self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)

        mtype = SIM_PARAMS['gpt_type']
        if mtype == 0:
            self.model = 'gpt-4o-mini-2024-07-18'
        elif mtype == 1:
            self.model = 'gpt-4-1106-preview'
        else:
            raise ValueError(f"Unsupported gpt_type: {mtype}")

    def call(self, messages, max_retries=5):
        print(messages)
        for attempt in range(max_retries):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0,
                    timeout=120,
                    max_tokens=2048
                )
                return resp.choices[0].message.content

            except (RateLimitError, APIError, APIConnectionError, Timeout) as e:  ####æ·»åŠ äº†ç¡çœ ç­–ç•¥ ï¼Œé¿å…GPTè¿ç»­å¤šæ¬¡è®¿é—®ï¼ŒGPTç½¢å·¥
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ç­–ç•¥ï¼š1s, 2s, 4s, 8s...
                print(f"âš ï¸ ç¬¬ {attempt+1} æ¬¡å°è¯•å¤±è´¥ï¼š{str(e)}ï¼Œå°†åœ¨ {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        raise RuntimeError("âŒ å¤šæ¬¡å°è¯•åä»æ— æ³•æˆåŠŸè°ƒç”¨ LLM æ¥å£ã€‚")


# âœ… æµ‹è¯•  test_llm_client.py
def test_llm_name():
    client = LLMClient()

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå–œæ¬¢ç®€æ´å›ç­”çš„æœºå™¨äººã€‚"},
        {"role": "user", "content": "ä½ çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ"}
    ]

    response = client.call(messages)
    print("æ¨¡å‹å›ç­”ï¼š", response)
    assert isinstance(response, str) and len(response) > 0
    print("âœ… æµ‹è¯•é€šè¿‡ï¼šæ¨¡å‹æˆåŠŸè¿”å›å†…å®¹ã€‚")

if __name__ == "__main__":
    test_llm_name()


# ####å¯ä»¥è°ƒç”¨æœ¬åœ°å¼€æºå¤§æ¨¡å‹çš„ä»£ç 
# import os
# import openai
# import json
# from openai import OpenAI
# from config import OPENAI_API_KEY, OPENAI_API_BASE, SIM_PARAMS
#
# from transformers import AutoTokenizer
# from vllm import LLM as VLLMClient, SamplingParams
#
# class LLMClient:
#     """
#     é€šç”¨ LLM è°ƒç”¨æ¨¡å—ï¼Œæ”¯æŒ OpenAI GPT ä¸ æœ¬åœ° vLLM æ¨¡å‹ï¼ˆå¦‚ LLaMAã€DeepSeekï¼‰
#     """
#
#     def __init__(self):
#         self.gpt_type = SIM_PARAMS['gpt_type']
#         self.temperature = SIM_PARAMS.get('temperature', 0)
#         self.max_tokens = SIM_PARAMS.get('max_tokens', 2048)
#
#         if self.gpt_type in [0, 1]:  # GPTç³»åˆ—ï¼šOpenAI
#             openai.api_key = OPENAI_API_KEY
#             self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
#             if self.gpt_type == 0:
#                 self.model = 'gpt-4o-mini-2024-07-18'
#             elif self.gpt_type == 1:
#                 self.model = 'gpt-4-1106-preview'
#
#         elif self.gpt_type == 2:  # æœ¬åœ° VLLM: LLaMA
#             self.model_path = SIM_PARAMS['LLaMA_local_model_path']  # ä¾‹å¦‚ "/data/llama3"
#             self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
#             self.client = VLLMClient(
#                 model=self.model_path,
#                 trust_remote_code=True,
#                 dtype="float16",
#                 max_model_len=2048
#             )
#         else:
#             raise ValueError(f"Unsupported gpt_type: {self.gpt_type}")
#
#     def call(self, messages):
#         print(messages)
#         if self.gpt_type in [0, 1]:
#             # OpenAI æ¨¡å‹è°ƒç”¨
#             print("ğŸ”µ ä½¿ç”¨ GPT æ¨¡å‹è°ƒç”¨ï¼š", self.model)
#             resp = self.client.chat.completions.create(
#                 model=self.model,
#                 messages=messages,
#                 temperature=self.temperature,
#                 timeout=120,
#                 max_tokens=self.max_tokens
#             )
#             return resp.choices[0].message.content
#
#         elif self.gpt_type == 2:
#             # vLLM æœ¬åœ°æ¨¡å‹è°ƒç”¨
#             # os.environ["CUDA_VISIBLE_DEVICES"] = SIM_PARAMS.get("cuda_device", "4")  # é»˜è®¤ä¸º GPU 0
#             print("ğŸŸ¢ ä½¿ç”¨æœ¬åœ° vLLM æ¨¡å‹ï¼š", self.model_path)
#
#             # å…¼å®¹ message æ ¼å¼è½¬æ¢ä¸º prompt string
#             role_map = {"system": "[System]", "user": "[User]", "assistant": "[Assistant]"}
#             prompt = ""
#             for msg in messages:
#                 role_tag = role_map.get(msg["role"], "")
#                 prompt += f"{role_tag} {msg['content']}\n"
#             prompt += "[Assistant] "
#
#             sampling_params = SamplingParams(
#                 temperature=self.temperature,
#                 top_p=0.9,
#                 max_tokens=self.max_tokens,
#                 stop=["[User]", "[System]", "[Assistant]"]
#             )
#
#             results = self.client.generate([prompt], sampling_params)
#             return results[0].outputs[0].text.strip()
#
#         else:
#             raise ValueError("Unsupported gpt_type")
#
# def test_llm_name():
#     client = LLMClient()
#
#     messages = [
#         {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå–œæ¬¢ç®€æ´å›ç­”çš„æœºå™¨äººã€‚"},
#         {"role": "user", "content": "ä½ çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ"}
#     ]
#
#     response = client.call(messages)
#     print("æ¨¡å‹å›ç­”ï¼š", response)
#     assert isinstance(response, str) and len(response.strip()) > 0
#     print("âœ… æµ‹è¯•é€šè¿‡ï¼šæ¨¡å‹æˆåŠŸè¿”å›å†…å®¹ã€‚")
#
#
# if __name__ == "__main__":
#     test_llm_name()


# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import openai
# from openai import OpenAI
# from config import OPENAI_API_KEY, OPENAI_API_BASE, SIM_PARAMS
#
#
# class LLMClient:
#     """
#     LLM æ¥å£è°ƒç”¨å°è£…æ¨¡å—
#     ç”¨äºç»Ÿä¸€å¤„ç† GPT / DeepSeek ç­‰æ¨¡å‹çš„è°ƒç”¨
#     """
#     def __init__(self):
#         mtype = SIM_PARAMS['gpt_type']
#
#         if mtype == 0:
#             openai.api_key = OPENAI_API_KEY
#             self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
#             self.model = 'gpt-4o-mini-2024-07-18'
#             self.provider = 'openai'
#
#         elif mtype == 1:
#             openai.api_key = OPENAI_API_KEY
#             self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
#             self.model = 'gpt-4-1106-preview'
#             self.provider = 'openai'
#
#         elif mtype == 2:
#             self.provider = 'deepseek'
#             model_dir = "/data/share_weight/DeepSeek-R1-Distill-Qwen-1.5B"
#             max_memory = {
#                 0: "10GiB",
#                 1: "10GiB",
#                 2: "10GiB",
#                 "cpu": "20GiB"
#             }
#             self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
#             self.model = AutoModelForCausalLM.from_pretrained(
#                 model_dir,
#                 torch_dtype=torch.float16,
#                 device_map="auto",
#                 max_memory=max_memory,
#                 trust_remote_code=True
#             )
#             self.model.eval()
#         else:
#             raise ValueError(f"Unsupported gpt_type: {mtype}")
#
#     def call(self, messages):
#         print("ğŸ“¨ è¾“å…¥æ¶ˆæ¯ï¼š", messages)
#
#         if self.provider == 'openai':
#             resp = self.client.chat.completions.create(
#                 model=self.model,
#                 messages=messages,
#                 temperature=0,
#                 timeout=120,
#                 max_tokens=2048
#             )
#             return resp.choices[0].message.content
#
#         elif self.provider == 'deepseek':
#             # æ„é€  promptï¼ˆå¿½ç•¥ systemï¼‰
#             prompt = ""
#             for msg in messages:
#                 if msg["role"] == "user":
#                     prompt += f"ç”¨æˆ·ï¼š{msg['content']}\n"
#                 elif msg["role"] == "assistant":
#                     prompt += f"åŠ©æ‰‹ï¼š{msg['content']}\n"
#                 elif msg["role"] == "system":
#                     continue
#             prompt += "åŠ©æ‰‹ï¼š"
#
#             inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
#             with torch.no_grad():
#                 output = self.model.generate(
#                     **inputs,
#                     max_new_tokens=256,
#                     do_sample=False,
#                     temperature=0
#                 )
#             decoded = self.tokenizer.decode(output[0], skip_special_tokens=True)
#             # æå–å›ç­”å†…å®¹
#             response = decoded[len(prompt):].strip()
#             return response
#
#
# def test_llm_name():
#     client = LLMClient()
#
#     messages = [
#         {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå–œæ¬¢ç®€æ´å›ç­”çš„æœºå™¨äººã€‚"},
#         {"role": "user", "content": "ä½ çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ"}
#     ]
#
#     response = client.call(messages)
#     print("ğŸ¤– æ¨¡å‹å›ç­”ï¼š", response)
#     assert isinstance(response, str) and len(response) > 0
#     print("âœ… æµ‹è¯•é€šè¿‡ï¼šæ¨¡å‹æˆåŠŸè¿”å›å†…å®¹ã€‚")
#
# if __name__ == "__main__":
#     test_llm_name()
